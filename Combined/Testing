
import cv2
from cvzone.HandTrackingModule import HandDetector
from cvzone.ClassificationModule import Classifier
import numpy as np
import math
from keras.models import load_model
from keras.preprocessing.image import img_to_array

# Initialize the video capture, hand detector, and classifier for sign language
cap = cv2.VideoCapture(0)
hand_detector = HandDetector(maxHands=2)  # Set maxHands to 2

# Paths for sign language model and labels
sign_model_path = r"C:\Users\jmana\OneDrive\Desktop\Keras\2\keras_model.h5"
sign_label_path = r"C:\Users\jmana\OneDrive\Desktop\Keras\2\labels.txt"
try:
    sign_classifier = Classifier(sign_model_path, sign_label_path)
except Exception as e:
    print(f"Error initializing sign language classifier: {e}")
    cap.release()
    cv2.destroyAllWindows()
    exit()

# Initialize face emotion detector
face_classifier = cv2.CascadeClassifier(r"C:\Users\jmana\OneDrive\Desktop\Face Emotion\haarcascade_frontalface_default.xml")
emotion_model = load_model(r"C:\Users\jmana\OneDrive\Desktop\Face Emotion\model.h5")
emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']

# Parameters
offset = 20
imgSize = 300
labels = ["Hello", "Yes", "No", "Help", "Thalaiva", "Thanks"]

# Set the window size (width x height)
window_width = 800
window_height = 600

# Create a named window with a specific size
cv2.namedWindow('Combined Output', cv2.WINDOW_NORMAL)
cv2.resizeWindow('Combined Output', window_width, window_height)

# Create a window for displaying detected sign and emotion
cv2.namedWindow('Detection Results', cv2.WINDOW_NORMAL)
cv2.resizeWindow('Detection Results', 400, 200)

# Initialize variables for detection results
last_signs = ["None", "None"]  # To store last signs detected for each hand
last_emotion = ""

while True:
    success, img = cap.read()
    if not success:
        break
    
    imgOutput = img.copy()
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    
    # Reset detection results
    detected_signs = ["None", "None"]
    detected_emotion = "None"

    # Hand detection and classification
    hands, img = hand_detector.findHands(img)
    if hands:
        for i, hand in enumerate(hands):
            x, y, w, h = hand['bbox']
            
            if w > 0 and h > 0:
                imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255
                imgCrop = img[y-offset:y + h + offset, x-offset:x + w + offset]

                if imgCrop.size != 0:
                    aspectRatio = h / w
                    if aspectRatio > 1:
                        k = imgSize / h
                        wCal = math.ceil(k * w)
                        imgResize = cv2.resize(imgCrop, (wCal, imgSize))
                        wGap = math.ceil((imgSize - wCal) / 2)
                        imgWhite[:, wGap: wCal + wGap] = imgResize
                    else:
                        k = imgSize / w
                        hCal = math.ceil(k * h)
                        imgResize = cv2.resize(imgCrop, (imgSize, hCal))
                        hGap = math.ceil((imgSize - hCal) / 2)
                        imgWhite[hGap: hCal + hGap, :] = imgResize

                    prediction, index = sign_classifier.getPrediction(imgWhite, draw=False)
                    detected_signs[i] = labels[index]
                    text_position = (x, y - 25)  # Adjust the position for the text
                    cv2.putText(imgOutput, detected_signs[i], text_position, cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)
                    cv2.rectangle(imgOutput, (x-offset, y-offset), (x + w + offset, y + h + offset), (0, 255, 0), 4)

    # Face emotion detection
    faces = face_classifier.detectMultiScale(gray)
    for (x, y, w, h) in faces:
        cv2.rectangle(imgOutput, (x, y), (x+w, y+h), (0, 255, 255), 2)
        roi_gray = gray[y:y+h, x:x+w]
        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation=cv2.INTER_AREA)
        
        if np.sum([roi_gray]) != 0:
            roi = roi_gray.astype('float') / 255.0
            roi = img_to_array(roi)
            roi = np.expand_dims(roi, axis=0)
            
            prediction = emotion_model.predict(roi)[0]
            detected_emotion = emotion_labels[prediction.argmax()]
            label_position = (x, y-5)
            cv2.putText(imgOutput, detected_emotion, label_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        else:
            cv2.putText(imgOutput, 'No Faces', (30, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    
    # Display the combined output
    cv2.imshow('Combined Output', imgOutput)
    
    # Display the detected signs and emotion in the separate window
    result_img = np.zeros((200, 500, 3), dtype=np.uint8)
    cv2.putText(result_img, f"Detected Sign 1: {detected_signs[0]}", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    cv2.putText(result_img, f"Detected Sign 2: {detected_signs[1]}", (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    cv2.putText(result_img, f"Detected Emotion: {detected_emotion}", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
    cv2.imshow('Detection Results', result_img)
    
    # Break the loop when 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release resources
cap.release()
cv2.destroyAllWindows()
